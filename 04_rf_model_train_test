####### Training ####### 

# Import 'train_set_final.xlsx' if needed
train_set_final <- readxl::read_excel("insert_your_pathway_to_file.xlsx")

# Apply correct class types to features.
# 'profile' must be factor; one-hot encoded and linguistic features
# must be numeric.
train_set_final$profile <- as.factor(train_set_final$profile)
train_set_final[, 1:33] <- lapply(train_set_final[, 1:33], as.numeric) 

# Confirm class types correctly assigned to features
str(train_set_final)

# Calculate class weights to for class imbalance
class_counts <- table(train_set_final$profile)
weights <- 1 / class_counts[train_set_final$profile]

weightedSummary <- function(data, lev = NULL, model = NULL) {
  # Ensure factor levels are consistent
  if (!is.null(lev)) data$obs <- factor(data$obs, levels = lev)
  
  # Class prevalence weights
  w <- as.numeric(table(data$obs) / nrow(data))
  
  # One-vs-all metrics for each class
  f1 <- sapply(lev, function(cl)
    caret::F_meas(factor(data$pred == cl, levels = c(FALSE, TRUE)),
                  factor(data$obs  == cl, levels = c(FALSE, TRUE)), beta = 1))
  
  sens <- sapply(lev, function(cl)
    caret::sensitivity(factor(data$pred == cl, levels = c(FALSE, TRUE)),
                       factor(data$obs  == cl, levels = c(FALSE, TRUE))))
  
  spec <- sapply(lev, function(cl)
    caret::specificity(factor(data$pred == cl, levels = c(FALSE, TRUE)),
                       factor(data$obs  == cl, levels = c(FALSE, TRUE))))
  
  acc <- mean(data$pred == data$obs)
  
  # Replace missing values (e.g., when no positive predictions)
  f1[is.na(f1)] <- 0
  sens[is.na(sens)] <- 0
  spec[is.na(spec)] <- 0
  
  # Weighted metrics (by class prevalence)
  wF1   <- sum(f1 * w)
  wSens <- sum(sens * w)
  wSpec <- sum(spec * w)
  
  # Macro (unweighted) metrics
  macroF1   <- mean(f1)
  macroSens <- mean(sens)
  macroSpec <- mean(spec)
  
  # Weighted SDs
  wF1_sd   <- sqrt(sum(w * (f1 - wF1)^2))
  wSens_sd <- sqrt(sum(w * (sens - wSens)^2))
  wSpec_sd <- sqrt(sum(w * (spec - wSpec)^2))
  
  # Return everything in one vector
  c(
    Weighted_F1 = wF1,
    Weighted_F1_SD = wF1_sd,
    Weighted_Sensitivity = wSens,
    Weighted_Sensitivity_SD = wSens_sd,
    Weighted_Specificity = wSpec,
    Weighted_Specificity_SD = wSpec_sd,
    Macro_F1 = macroF1,
    Macro_Sensitivity = macroSens,
    Macro_Specificity = macroSpec,
    Accuracy = acc
  )
}

# Define resampling and evaluation settings
set.seed(123)                         # set a reproducible random seed
ctrl <- caret::trainControl(
  method = "cv",                      # k-fold cross-validation
  number = 10,                        # k = 10 folds
  classProbs = TRUE,                  # compute class probabilities
  summaryFunction = weightedSummary,  # weighted evaluation
  savePredictions = "final",
  search = "grid")                    # use grid search

# Train model
set.seed(123)
caret::rf_fit <- train(
  profile ~ .,                        # outcome predicted by all other features
  data = train_set_final,  
  method = "rf",                      # random forest algorithm
  trControl = ctrl,
  tuneLength = 10,                    # parameter space
  metric = "Macro_F1",                # performance metric
  maximize = TRUE,
  weights = weights)                  # apply class weights for class imbalance


# Remove extraneous metrics from training results
rf_fit$results <- rf_fit$results[, !names(rf_fit$results) %in% c(
  "Weighted_F1", 
  "Weighted_F1_SD", 
  "Weighted_Sensitivity", 
  "Weighted_Sensitivity_SD", 
  "Weighted_Specificity", 
  "Weighted_Specificity_SD",
  "Macro_Sensitivity",
  "Macro_Specificity",
  "Accuracy",
  "Weighted_F1SD",
  "Weighted_F1_SDSD",
  "Weighted_SensitivitySD",
  "Weighted_Sensitivity_SDSD",
  "Weighted_SpecificitySD",
  "Weighted_Specificity_SDSD",
  "Macro_SensitivitySD",
  "Macro_SpecificitySD",
  "AccuracySD"
)]

# Print results of model training
print(rf_fit)

# Check variable importance
caret::varImp(rf_fit)
plot(caret::varImp(rf_fit))


####### Testing ####### 

# Import 'test_set_final.xlsx' if needed
test_set_final <- readxl::read_excel("insert_your_pathway_to_file.xlsx")

# Apply correct class types to features.
# 'profile' must be factor; one-hot encoded and linguistic features
# must be numeric.
test_set_final$profile <- as.factor(test_set_final$profile)
test_set_final[, 1:33] <- lapply(test_set_final[, 1:33], as.numeric) 

# Confirm class types correctly assigned to features
str(test_set_final)

# Apply trained model to test set 
predictions <- stats::predict(rf_fit, newdata = test_set_final)

# Create df that includes both predicted and actual labels
results <- test_set_final %>% dplyr::mutate(pred = predictions)

# calculate weighted averages for precision, recall, and F1

precision_weighted <- yardstick::precision(results, truth = profile, estimate = pred,
                      estimator = "macro_weighted")

recall_weighted <- yardstick::recall(results, truth = profile, estimate = pred,
                   estimator = "macro_weighted")

f1_weighted <- yardstick::f_meas(results, truth = profile, estimate = pred,
               estimator = "macro_weighted")

# Combine all weighted metrics into one table for easy viewing 
all_test_metrics <- bind_rows(precision_weighted,
                              recall_weighted,
                              f1_weighted)
# View weighted metrics                        
print(all_test_metrics)
